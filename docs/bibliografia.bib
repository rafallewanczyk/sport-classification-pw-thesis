% Artykuł w recenzowanym czasopiśmie.
@article{szczypiorski2015,
    author    = "Szczypiorski, K. and Janicki, A. and Wendzel, S",
    title     = "{T}he {G}ood, {T}he {B}ad {A}nd {T}he {U}gly: {E}valuation of {W}i-{F}i {S}teganography",
    journal   = "Journal of Communications",
    volume    = "10",
    number    = "10",
    pages     = "747--752",
    publisher = "Journal of Communications (JCM)",
    year      = "2015",
}

% Książka.
@book{goossens93,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The LaTeX Companion",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts",
    year      = "1993",
}

% Fragment książki (np. zakres stron).
@inbook{wang97,
    author      = "Hao Wang",
    title       = "A Logical Journey: From G{\"o}del to Philosophy.",
    publisher   = "A Bradford Book",
    pages       = "316",
    year        = "1997"
}

% Fragment książki (np. esej), posiadający własny tytuł.
@incollection{goedel95,
    author      = "Kurt G{\"o}del",
    title       = "Texts relating to the ontological proof",
    booktitle   = "Unpublished Essays and Lectures",
    publisher   = "Oxford University Press",
    pages       = "429--437",
    year        = "1995",
}

% Publikacja konferencyjna.
@inproceedings{benzmuller2014,
    author       = "{Ch}. Benzmuller and B. W. Paleo",
    title        = "Automating {G\"o}del’s {O}ntological {P}roof of {G}od’s {E}xistence with {H}igher-order {A}utomated {T}heorem {P}rovers",
    booktitle    = "European	Conference on Artificial Intelligence",
    publisher    = "IOS Press",
    howpublished = "Dostęp zdalny (10.04.2019): \url{http://page.mi.fu-berlin.de/cbenzmueller/papers/C40.pdf}",
    year         = "2014",
}

% Raport techniczny.
@techreport{duqu2011,
    author      = "Bencsáth, B. and Pék, G. and Buttyán, L. and Félegyházi M.",
    title       = "{D}uqu: {A} {S}tuxnet-like malware found in the wild",
    institution = "Laboratory of Cryptography and System Security, Hungary",
    year        = "2011"
}

% Specyfikacja techniczna.
@manual{shs2015,
    title        = "{FIPS} 180-4: {S}ecure {H}ash {S}tandard ({SHS})",
    howpublished = "Dostęp zdalny (13.03.2019): \url{https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf}",
    year         = "2015"
}

% Praca magisterska.
@mastersthesis{wozniak2018,
    author = "Woźniak, Piotr",
    title  = "{P}rogramowanie kwadratowe w usuwaniu efektu rozmycia ruchu w fotografii cyfrowej",
    school  = "Wydział Elektroniki i Technik Informacyjnych, Politechnika Warszawska",
    year   = "2018",
}

% Nieopublikowany artykuł, dostępny np. tylko w internecie.
@unpublished{koons2005,
    author = "Koons, Robert C.",
    title  = "{S}obel on {G\"o}del’s {O}ntological {P}roof",
    note   = "Dostęp zdalny (25.04.2019): \url{http://www.robkoons.net/media/69b0dd04a9d2fc6dffff80b4ffffd524.pdf}",
    year   = "2005"
}

@misc{sandvine,
    author       = "Sandvine",
    title        = "{G}lobal Internet Phenomena Report",
    howpublished = "Dostęp zdalny (01.06.2023): \url{https://www.sandvine.com/global-internet-phenomena-report-2023}",
    year         = "2023"
}
@misc{automl,
    author       = "Google",
    title        = "AutoML",
    howpublished = "Dostęp zdalny (01.06.2023): \url{https://cloud.google.com/automl}",
    year         = "2023"
}
@misc{yolov7-pose,
    author       = "Rizwan Munawar",
    title        = "YOLOv7 Pose",
    howpublished = "Dostęp zdalny (01.06.2023): \url{https://github.com/RizwanMunawar/yolov7-pose-estimation}",
    year         = "2023"
}
@misc{deepsort,
    author       = "Deshwal Mahesh",
    title        = "YOLOv7 DeepSORT",
    howpublished = "Dostęp zdalny (01.06.2023): \url{https://github.com/deshwalmahesh/yolov7-deepsort-tracking}",
    year         = "2023"
}
@misc{yt_dataset, 
    author       = "UCF YouTube Action Data Set",
    title        = "{U}CF YouTube Action Data Set",
    howpublished = "Dostęp zdalny (01.06.2023): \url{https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php}",
    year         = "2023"
}
@generic{Wu2021,
   abstract = {Human group activity recognition (GAR) has attracted significant attention from computer vision researchers due to its wide practical applications in security surveillance, social role understanding and sports video analysis. In this paper, we give a comprehensive overview of the advances in group activity recognition in videos during the past 20 years. First, we provide a summary and comparison of 11 GAR video datasets in this field. Second, we survey the group activity recognition methods, including those based on handcrafted features and those based on deep learning networks. For better understanding of the pros and cons of these methods, we compare various models from the past to the present. Finally, we outline several challenging issues and possible directions for future research. From this comprehensive literature review, readers can obtain an overview of progress in group activity recognition for future studies.},
   author = {Li Fang Wu and Qi Wang and Meng Jian and Yu Qiao and Bo Xuan Zhao},
   doi = {10.1007/s11633-020-1258-8},
   issn = {17518520},
   issue = {3},
   journal = {International Journal of Automation and Computing},
   keywords = {Group activity recognition (GAR),computer vision,human activity recognition,scene understanding,video analysis},
   month = {6},
   pages = {334-350},
   publisher = {Chinese Academy of Sciences},
   title = {A Comprehensive Review of Group Activity Recognition in Videos},
   volume = {18},
   year = {2021},
}
@article{Szkielety,
   author = {Tiberio Uricchio, Lorenzo Seidenari, Alberto del Bimbo Fabio Zappardino},
   isbn = {9781728188089},
   publisher = {IEEE},
   title = {Learning Group Activities from Skeletons withoutIndividual Action Labels},
   year = {2021},
}
@article{Ibrahim2015,
   abstract = {In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of in- dividual people in a sequence and another LSTM model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods.},
   author = {Moustafa Ibrahim and Srikanth Muralidharan and Zhiwei Deng and Arash Vahdat and Greg Mori},
   month = {11},
   title = {A Hierarchical Deep Temporal Model for Group Activity Recognition},
   url = {http://arxiv.org/abs/1511.06040},
   year = {2015},
}
@article{openpose,
   abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset th weaort have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
   author = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh},
   month = {12},
   title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
   year = {2018},
}
@report{alexnet,
   abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
   author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E Hinton},
   title = {ImageNet Classification with Deep Convolutional Neural Networks},
   url = {http://code.google.com/p/cuda-convnet/},
}
@report{collective_ds,
   abstract = {In this paper we present a new framework for pedestrian action categorization. Our method enables the classification of actions whose semantic can be only analyzed by looking at the collective behavior of pedestrians in the scene. Examples of these actions are waiting by a street intersection versus standing in a queue. To that end, we exploit the spatial distribution of pedestrians in the scene as well as their pose and motion for achieving robust action classification. Our proposed solution employs extended Kalman filtering for tracking of detected pedestrians in 2D 1/2 scene coordinates as well as camera parameter and horizon estimation for tracker filtering and stabilization. We present a local spatio-temporal descriptor effective in capturing the spatial distribution of pedestrians over time as well as their pose. This descriptor captures pedestrian activity while requiring no high level scene understanding. Our work is tested against highly challenging real world pedestrian video sequences captured by low resolution hand held cameras. Experimental results on a 5-class action dataset indicate that our solution: i) is effective in classifying collective pedestrian activities; ii) is tolerant to challenging real world conditions such as variation in illumination , scale, viewpoint as well as partial occlusion and background motion; iii) outperforms state-of-the art action classification techniques.},
   author = {Wongun Choi and Khuram Shahid and Silvio Savarese},
   title = {What are they doing? : Collective Activity Classification Using Spatio-Temporal Relationship Among People},
}

@article{Ullah2017,
   abstract = {Recurrent neural network (RNN) and long short-Term memory (LSTM) have achieved great success in processing sequential multimedia data and yielded the state-of-The-Art results in speech recognition, digital signal processing, video processing, and text data analysis. In this paper, we propose a novel action recognition method by processing the video data using convolutional neural network (CNN) and deep bidirectional LSTM (DB-LSTM) network. First, deep features are extracted from every sixth frame of the videos, which helps reduce the redundancy and complexity. Next, the sequential information among frame features is learnt using DB-LSTM network, where multiple layers are stacked together in both forward pass and backward pass of DB-LSTM to increase its depth. The proposed method is capable of learning long term sequences and can process lengthy videos by analyzing features for a certain time interval. Experimental results show significant improvements in action recognition using the proposed method on three benchmark data sets including UCF-101, YouTube 11 Actions, and HMDB51 compared with the state-of-The-Art action recognition methods.},
   author = {Amin Ullah and Jamil Ahmad and Khan Muhammad and Muhammad Sajjad and Sung Wook Baik},
   doi = {10.1109/ACCESS.2017.2778011},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Action recognition,convolution neural network,deep bidirectional long short-Term memory,deep learning,recurrent neural network},
   month = {11},
   pages = {1155-1166},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Action Recognition in Video Sequences using Deep Bi-Directional LSTM with CNN Features},
   volume = {6},
   year = {2017},
}
@article{mobilenet,
   abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
   author = {Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
   month = {4},
   title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
   year = {2017},
}
@article{yolov7,
   abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
   author = {Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
   month = {7},
   title = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
   year = {2022},
}
@inproceedings{svw,
   author = {Seyed Morteza Safdarnejad and Xiaoming Liu and Lalita Udpa and Brooks Andrus and John Wood and Dean Craven},
   doi = {10.1109/FG.2015.7163105},
   isbn = {978-1-4799-6026-2},
   journal = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
   month = {5},
   pages = {1-7},
   publisher = {IEEE},
   title = {Sports Videos in the Wild (SVW): A video dataset for sports analysis},
   year = {2015},
}
@article{cnn-joey,
   author = {Joey Asperger and Austin Poore},
   title = {Convolutional Neural Networks for Classification of Noisy Sports Videos},
}
@article{inception-resnet-v2,
   abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
   author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
   month = {2},
   title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
   year = {2016},
}
@report{kumar,
   author = {by Santanu Datta and Kumar Sankar Ray},
   title = {Sports Video Action Recognition},
   year = {2019},
}

@article{vgg16,
   abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
   author = {Karen Simonyan and Andrew Zisserman},
   month = {9},
   title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
   year = {2014},
}
